{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstration of how points can be matched between\n",
    "# multiple videos of a single striking ant\n",
    "# this is temporary and/or needs to be cleaned up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hsflfm.processing import Aligner\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from hsflfm.processing import match_points_between_images, get_point_locations\n",
    "from hsflfm.util import procrustes_analysis\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specimen_name = \"20220422_OB_1\"\n",
    "#specimen_name = \"20240418_OB_1_rp\"\n",
    "#specimen_name = \"20240506_OB_7\"\n",
    "#specimen_name = \"20220422_OB_1\"\n",
    "#specimen_name = \"20240507_OB_2\"\n",
    "#specimen_name = \"20240506_OB_1\"\n",
    "\n",
    "\n",
    "\n",
    "aligner = Aligner(specimen_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligner.stored_point_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligner.data_manager.strike_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.signal\n",
    "\n",
    "def cross_image(im1_gray, im2_gray):\n",
    "   # get rid of the color channels by performing a grayscale transform\n",
    "   # the type cast into 'float' is to avoid overflows\n",
    "   #im1_gray = np.sum(im1.astype('float'), axis=2)\n",
    "   #im2_gray = np.sum(im2.astype('float'), axis=2)\n",
    "\n",
    "   # get rid of the averages, otherwise the results are not good\n",
    "   im1_gray -= np.mean(im1_gray)\n",
    "   im2_gray -= np.mean(im2_gray)\n",
    "\n",
    "   # calculate the correlation image; note the flipping of onw of the images\n",
    "   corr_image = scipy.signal.fftconvolve(im1_gray, im2_gray[::-1,::-1], mode='same')\n",
    "   unraveled = np.unravel_index(np.argmax(corr_image), corr_image.shape)\n",
    "   middle = corr_image.shape[0] / 2, corr_image.shape[1] / 2\n",
    "   return unraveled[0] - middle[0], unraveled[1] - middle[1]\n",
    "\n",
    "def align_strike(\n",
    "        self,\n",
    "        strike_number,\n",
    "        start_strike=None,\n",
    "        match_sequentially=True,\n",
    "        flow_parameters=None,\n",
    "        reshift=True,\n",
    "    ):\n",
    "        run = True\n",
    "        bad_numbers = []\n",
    "\n",
    "        threshold = self.alignment_settings[\"point_error_cutoff\"]\n",
    "\n",
    "        if not match_sequentially and start_strike is None:\n",
    "            start_strike = 1\n",
    "        elif match_sequentially and start_strike is None:\n",
    "            start_strike = max(1, strike_number - 1)\n",
    "\n",
    "        if start_strike not in self.stored_match_points:\n",
    "            self.align_strike(start_strike, match_sequentially=match_sequentially)\n",
    "\n",
    "        strike_match_points = {}\n",
    "        prev_match_points = self.stored_match_points[start_strike]\n",
    "        prev_images = self.data_manager.get_start_images(strike_number=start_strike)\n",
    "        new_images = self.data_manager.get_start_images(strike_number)\n",
    "\n",
    "        for key, prev_image in prev_images.items():\n",
    "            new_image = new_images[key]\n",
    "            pmp = np.asarray(prev_match_points[key])[:, :2]\n",
    "            \n",
    "            if reshift:\n",
    "                plt.figure()\n",
    "                plt.imshow(prev_image - new_image)\n",
    "                plt.scatter(pmp[:, 1], pmp[:, 0])\n",
    "                plt.title(\"original\")\n",
    "                # this could get thrown in by consistent background\n",
    "                # so we'll crop in around the actual points\n",
    "                # buffers = {\n",
    "                #     0: [65, 10, 35, 10],\n",
    "                #     1: [0, 45, 0, 15],\n",
    "                #     2: [45, 10,35, 10]\n",
    "                # }\n",
    "                # b = buffers[key]\n",
    "\n",
    "                rough_shift = (0, 0)#(-50, -15)\n",
    "                translation_matrix = np.float32([[1, 0, rough_shift[0]], [0, 1, rough_shift[1]]])\n",
    "                prev_image = cv2.warpAffine(\n",
    "                    prev_image.T, translation_matrix, prev_image.shape[:2]\n",
    "                ).T\n",
    "\n",
    "                plt.figure()\n",
    "                plt.imshow(prev_image - new_image)\n",
    "                plt.title(\"rough aligned\")\n",
    "\n",
    "                pmp = pmp + rough_shift\n",
    "                \n",
    "                b = [15, 15, 15, 15]\n",
    "                minx = int(np.min(pmp[:, 0]) - b[0]) \n",
    "                maxx = int(np.max(pmp[:, 0]) + b[1]) \n",
    "                miny = int(np.min(pmp[:, 1]) - b[2]) \n",
    "                maxy = int(np.max(pmp[:, 1]) + b[3]) \n",
    "                print(minx, maxx, miny, maxy)\n",
    "                minx = max(minx, 0)\n",
    "                maxx = min(maxx, prev_image.shape[0]) \n",
    "                miny = max(miny, 0)\n",
    "                maxy = min(maxy, prev_image.shape[1])\n",
    "                \n",
    "                i0 = prev_image[minx:maxx, miny:maxy].copy()\n",
    "                i1 = new_image[minx:maxx, miny:maxy].copy()\n",
    "\n",
    "                plt.figure()\n",
    "                plt.imshow(i0 - i1)\n",
    "                plt.title(\"cropped\")\n",
    "                \n",
    "                shiftx, shifty = cross_image(i1, i0)\n",
    "                print(shiftx, shifty)\n",
    "\n",
    "                # SOMETHING IS WEIRD\n",
    "                # with the order we're using either shiftx, shifty or rough_shift\n",
    "                #shiftx = 50 \n",
    "                #shifty = -10\n",
    "                translation_matrix = np.float32([[1, 0, shiftx], [0, 1, shifty]])\n",
    "                prev_image = cv2.warpAffine(\n",
    "                    prev_image.T, translation_matrix, prev_image.shape[:2]\n",
    "                ).T\n",
    "                pmp = pmp + [shiftx, shifty]\n",
    "\n",
    "            new_points = match_points_between_images(\n",
    "                prev_image,\n",
    "                new_image,\n",
    "                pmp,\n",
    "                flow_parameters=flow_parameters,\n",
    "            )\n",
    "\n",
    "            plt.figure()\n",
    "            plt.title(\"fine aligned\")\n",
    "            plt.imshow(prev_image - new_image)\n",
    "\n",
    "            strike_match_points[key] = new_points\n",
    "\n",
    "        # convert to camera points\n",
    "        strike_locations = get_point_locations(self.system, strike_match_points)\n",
    "        prev_strike_locations = get_point_locations(self.system, prev_match_points)\n",
    "\n",
    "        strike_point_indices = np.arange(len(prev_match_points[2]))\n",
    "\n",
    "        while run:\n",
    "            # fit a transformation between the two\n",
    "            A_cam2_to_cam1, _, transformed_points = procrustes_analysis(\n",
    "                strike_locations[strike_point_indices],\n",
    "                prev_strike_locations[strike_point_indices],\n",
    "                allow_scale=False,\n",
    "            )\n",
    "\n",
    "            # look at error\n",
    "            diff = np.linalg.norm(\n",
    "                prev_strike_locations[strike_point_indices] - transformed_points,\n",
    "                axis=1,\n",
    "            )\n",
    "            bp = np.where(diff > threshold)[0]\n",
    "\n",
    "            if len(bp) > 0:\n",
    "                worst_index = np.argsort(diff)[-1]\n",
    "                bad_numbers.append(int(strike_point_indices[worst_index]))\n",
    "                strike_point_indices = np.delete(strike_point_indices, worst_index)\n",
    "            else:\n",
    "                run = False\n",
    "\n",
    "        for key, item in strike_match_points.items():\n",
    "            strike_match_points[key] = np.asarray(item)[strike_point_indices]\n",
    "\n",
    "        self.stored_match_points[strike_number] = strike_match_points\n",
    "        prev_point_numbers = self.stored_point_numbers[start_strike]\n",
    "\n",
    "        point_numbers = prev_point_numbers[strike_point_indices]\n",
    "        self.stored_point_numbers[strike_number] = point_numbers\n",
    "\n",
    "        return A_cam2_to_cam1, strike_match_points, point_numbers, bad_numbers, start_strike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_parameters = {\n",
    "    \"pyr_scale\": 0.9,\n",
    "    \"levels\": 15,\n",
    "    \"winsize\": 5,\n",
    "    \"iterations\": 5,\n",
    "    \"poly_n\": 3,\n",
    "    \"poly_sigma\": 0.8,\n",
    "    \"flags\": 0,\n",
    "}\n",
    "\n",
    "# flow_parameters = {\n",
    "#     \"pyr_scale\": 0.5,\n",
    "#     \"levels\": 15,\n",
    "#     \"winsize\": 35,\n",
    "#     \"iterations\": 15,\n",
    "#     \"poly_n\": 7,\n",
    "#     \"poly_sigma\": 1.2,\n",
    "#     \"flags\": 0,\n",
    "# }\n",
    "#flow_parameters = None\n",
    "s = 7\n",
    "res = align_strike(\n",
    "    aligner, s, reshift=True, flow_parameters=flow_parameters, start_strike=None,\n",
    "    #rough_flow_parameters=rough_flow_parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(aligner.stored_point_numbers[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(aligner.stored_point_numbers[s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_num = 2\n",
    "sn0 = 6\n",
    "sn1 = 7\n",
    "\n",
    "\n",
    "\n",
    "i0 = aligner.data_manager.get_start_images(strike_number=sn0)[cam_num]\n",
    "i1 = aligner.data_manager.get_start_images(strike_number=sn1)[cam_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2)\n",
    "axes[0].imshow(i0, cmap=\"gray\")\n",
    "axes[1].imshow(i1, cmap=\"gray\")\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = i0 - i1\n",
    "p0 = aligner.stored_match_points[sn0][cam_num]\n",
    "p1 = aligner.stored_match_points[sn1][cam_num]\n",
    "plt.imshow(diff)\n",
    "plt.scatter(p0[:, 1], p0[:, 0], s=5, color=\"white\", alpha=0.5)\n",
    "plt.scatter(p1[:, 1], p1[:, 0], color=\"black\", s=5, alpha=0.5)\n",
    "\n",
    "# plt.xlim(60, 130)\n",
    "# plt.ylim(200, 100)\n",
    "\n",
    "fig, (ax0, ax1) = plt.subplots(1, 2)\n",
    "ax0.imshow(i0, cmap='gray')\n",
    "ax0.scatter(p0[:, 1], p0[:, 0], s=5, color='white')\n",
    "ax0.set_xticks([])\n",
    "ax0.set_yticks([])\n",
    "ax0.set_ylim(200, 50)\n",
    "ax0.set_xlim(30, 100)\n",
    "\n",
    "ax1.imshow(i1, cmap='gray')\n",
    "ax1.scatter(p1[:, 1], p1[:, 0], s=5, color='black')\n",
    "ax1.set_xticks([])\n",
    "ax1.set_yticks([])\n",
    "ax1.set_ylim(200, 50)\n",
    "ax1.set_xlim(30, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# potential ants of concern\n",
    "# 20240506_OB_1\n",
    "# 20240417 OB 1 and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i0c = i0[50:190, 10:120]\n",
    "i1c = i1[50:190, 10:120]\n",
    "print(cross_image(i0c, i1c))\n",
    "print(cross_image(i0, i1))\n",
    "\n",
    "plt.imshow(i0c)\n",
    "plt.figure()\n",
    "plt.imshow(i1c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlate_images(im1_gray, im2_gray):\n",
    "    # get rid of the averages, otherwise the results are not good\n",
    "    im1_gray -= np.mean(im1_gray)\n",
    "    im2_gray -= np.mean(im2_gray)\n",
    "\n",
    "    # calculate the correlation image; note the flipping of onw of the images\n",
    "    corr_image = scipy.signal.fftconvolve(im1_gray, im2_gray[::-1, ::-1], mode=\"same\")\n",
    "    return corr_image\n",
    "from skimage.transform import warp_polar\n",
    "def get_angular_difference(im0, im1):\n",
    "    i0p = warp_polar(im0) \n",
    "    i1p = warp_polar(im1)\n",
    "\n",
    "    i1p -= np.mean(i1p)\n",
    "    i0p -= np.mean(i0p)\n",
    "    corr_image = scipy.signal.fftconvolve(i0p, i1p[::-1, ::-1], mode=\"same\")\n",
    "    unraveled = np.unravel_index(np.argmax(corr_image), corr_image.shape)\n",
    "    return 180 - unraveled[0]\n",
    "\n",
    "\n",
    "\n",
    "i0c = i0[50:190, 10:120]\n",
    "i1c = i1[50:190, 10:120]\n",
    "plt.imshow(i0c - i1c)\n",
    "\n",
    "angle = get_angular_difference(i0c, i1c)\n",
    "print(angle)\n",
    "\n",
    "from skimage.transform import warp, AffineTransform\n",
    "transform = AffineTransform(rotation=np.deg2rad(-angle))\n",
    "i0c = warp(i0c, transform)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(i0c - i1c)\n",
    "\n",
    "shifty, shiftx = cross_image(i1c, i0c)\n",
    "print(shiftx, shifty)\n",
    "\n",
    "from skimage.transform import warp, AffineTransform\n",
    "transform = AffineTransform(translation=(-shiftx, -shifty))#, rotation=np.deg2rad(-angle))\n",
    "\n",
    "i0ct = warp(i0c, transform)\n",
    "plt.figure()\n",
    "plt.imshow(i0ct - i1c)\n",
    "\n",
    "print(cross_image(i0ct, i1c))\n",
    "print(get_angular_difference(i0ct, i1c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def estimate_affine_transform(img1, img2):\n",
    "    \"\"\"\n",
    "    Estimate an affine transform between two images using OpenCV feature matching.\n",
    "\n",
    "    Parameters:\n",
    "        img1 (ndarray): Reference image.\n",
    "        img2 (ndarray): Image to align.\n",
    "\n",
    "    Returns:\n",
    "        ndarray: The affine transformation matrix (2x3) or None if estimation fails.\n",
    "    \"\"\"\n",
    "    # Detect ORB keypoints and descriptors\n",
    "    orb = cv2.ORB_create()\n",
    "    kp1, des1 = orb.detectAndCompute(img1, None)\n",
    "    kp2, des2 = orb.detectAndCompute(img2, None)\n",
    "\n",
    "    # Match descriptors using BFMatcher\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "    matches = bf.match(des1, des2)\n",
    "    matches = sorted(matches, key=lambda x: x.distance)\n",
    "\n",
    "    # Extract matched keypoints\n",
    "    src_pts = np.float32([kp1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "    dst_pts = np.float32([kp2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "\n",
    "    # Estimate affine transform\n",
    "    M, inliers = cv2.estimateAffinePartial2D(src_pts, dst_pts)\n",
    "    return M\n",
    "\n",
    "# Example Usage:\n",
    "# img1 = cv2.imread('image1.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "# img2 = cv2.imread('image2.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "# M = estimate_affine_transform(img1, img2)\n",
    "\n",
    "# Apply the transform\n",
    "# if M is not None:\n",
    "#     rows, cols = img1.shape\n",
    "#     aligned_img = cv2.warpAffine(img2, M, (cols, rows))\n",
    "#     cv2.imshow(\"Aligned Image\", aligned_img)\n",
    "#     cv2.waitKey(0)\n",
    "#     cv2.destroyAllWindows()\n",
    "\n",
    "M = estimate_affine_transform(i0c.astype(np.uint8), i1c.astype(np.uint8))\n",
    "\n",
    "cols, rows = i1c.shape\n",
    "plt.imshow(cv2.warpAffine(i1c, M, (rows, cols)) - i0c)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlate_images(im1_gray, im2_gray):\n",
    "    # get rid of the averages, otherwise the results are not good\n",
    "    im1_gray -= np.mean(im1_gray)\n",
    "    im2_gray -= np.mean(im2_gray)\n",
    "\n",
    "    # calculate the correlation image; note the flipping of onw of the images\n",
    "    corr_image = scipy.signal.fftconvolve(im1_gray, im2_gray[::-1, ::-1], mode=\"same\")\n",
    "    return corr_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_img = correlate_images(i0, i1)\n",
    "print(corr_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(corr_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unravel_index(np.argmax(corr_img), corr_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cross_image(i0c, i1c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.fft import fft2, ifft2, fftshift\n",
    "from scipy.ndimage import fourier_shift\n",
    "from skimage.transform import warp, AffineTransform\n",
    "\n",
    "def approximate_affine_transform(img1, img2):\n",
    "    \"\"\"\n",
    "    Estimate an approximate affine transform between two images using Fourier space.\n",
    "\n",
    "    Parameters:\n",
    "        img1 (ndarray): The first input image (reference).\n",
    "        img2 (ndarray): The second input image to align to the reference.\n",
    "\n",
    "    Returns:\n",
    "        AffineTransform: An AffineTransform object with the estimated parameters.\n",
    "    \"\"\"\n",
    "    # Ensure images are the same size\n",
    "    assert img1.shape == img2.shape, \"Images must be of the same shape.\"\n",
    "\n",
    "    # Compute Fourier transforms of both images\n",
    "    F1 = fft2(img1)\n",
    "    F2 = fft2(img2)\n",
    "    \n",
    "    # Compute cross-power spectrum\n",
    "    R = (F1 * F2.conj()) / np.abs(F1 * F2.conj())\n",
    "    R[np.isnan(R)] = 0  # Handle division by zero\n",
    "    corr = np.abs(ifft2(R))\n",
    "\n",
    "    plt.imshow(corr)\n",
    "    \n",
    "    # Find peak correlation for translation\n",
    "    shift = np.array(np.unravel_index(np.argmax(corr), corr.shape)) - np.array(img1.shape) // 2\n",
    "\n",
    "    # Adjust for wrap-around\n",
    "    shift = np.mod(shift + np.array(img1.shape) // 2, img1.shape) - np.array(img1.shape) // 2\n",
    "\n",
    "    # Perform log-polar transformation to estimate rotation and scale\n",
    "    log_polar_img1 = warp_to_log_polar(img1)\n",
    "    log_polar_img2 = warp_to_log_polar(img2)\n",
    "    \n",
    "    # Repeat phase correlation in log-polar space\n",
    "    F1_log = fft2(log_polar_img1)\n",
    "    F2_log = fft2(log_polar_img2)\n",
    "    R_log = (F1_log * F2_log.conj()) / np.abs(F1_log * F2_log.conj())\n",
    "    R_log[np.isnan(R_log)] = 0\n",
    "    corr_log = np.abs(ifft2(R_log))\n",
    "    \n",
    "    log_shift = np.unravel_index(np.argmax(corr_log), corr_log.shape)\n",
    "    scale_factor = np.exp(log_shift[0] / corr_log.shape[0])\n",
    "    rotation_angle = -log_shift[1] / corr_log.shape[1] * 360\n",
    "\n",
    "    # Construct the affine transform\n",
    "    transform = AffineTransform(scale=(scale_factor, scale_factor), rotation=np.deg2rad(rotation_angle), translation=shift)\n",
    "\n",
    "    return transform\n",
    "\n",
    "def warp_to_log_polar(image):\n",
    "    \"\"\"\n",
    "    Helper function to perform log-polar transformation.\n",
    "    \"\"\"\n",
    "    from skimage.transform import warp_polar\n",
    "    center = (image.shape[0] // 2, image.shape[1] // 2)\n",
    "    return warp_polar(image, center=center, scaling='log')\n",
    "\n",
    "# Example Usage:\n",
    "# img1 = ... # Reference image as a 2D NumPy array\n",
    "# img2 = ... # Image to align as a 2D NumPy array\n",
    "# transform = approximate_affine_transform(img1, img2)\n",
    "# print(transform.params)  # Matrix representation of the affine transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import warp_polar\n",
    "plt.imshow(warp_polar(i0c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(i0.shape, warp_polar(i0c).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hsflfm.util import cross_image\n",
    "shifty, shiftx = cross_image(i1c, i0c)\n",
    "print(shiftx, shifty)\n",
    "\n",
    "translation_matrix = np.float32([[1, 0, shifty], [0, 1, shiftx]])\n",
    "i0_shifted = cv2.warpAffine(\n",
    "    i0c.T, translation_matrix, i0c.shape[:2]\n",
    ").T\n",
    "\n",
    "i0p = warp_polar(i0_shifted) \n",
    "i1p = warp_polar(i1c)\n",
    "im1_gray = i1p \n",
    "im2_gray = i0p\n",
    "im1_gray -= np.mean(im1_gray)\n",
    "im2_gray -= np.mean(im2_gray)\n",
    "\n",
    "# calculate the correlation image; note the flipping of onw of the images\n",
    "corr_image = scipy.signal.fftconvolve(im1_gray, im2_gray[::-1, ::-1], mode=\"same\")\n",
    "\n",
    "plt.imshow(corr_image)\n",
    "unraveled = np.unravel_index(np.argmax(corr_image), corr_image.shape)\n",
    "print(unraveled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(i0_shifted - i1c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(i0c - i1c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = AffineTransform(rotation=np.deg2rad(-2))\n",
    "from skimage.transform import warp \n",
    "i0cr = warp(i0_shifted, transform)\n",
    "plt.imshow(i0cr)\n",
    "plt.figure()\n",
    "plt.imshow(i0cr - i1c)\n",
    "plt.figure()\n",
    "plt.imshow(i0_shifted - i1c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approximate_affine_transform(i0c, i1c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hsflfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
