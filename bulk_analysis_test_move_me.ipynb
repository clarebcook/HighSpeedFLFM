{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hsflfm.analysis import (\n",
    "    ResultManager,\n",
    "    ResultPlotter,\n",
    "    BulkAnalyzer,\n",
    "    convert_to_percentile,\n",
    "    get_random_percentile_index,\n",
    "    sort_by_camera,\n",
    "    get_percentiles\n",
    ")\n",
    "\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from hsflfm.util import load_dictionary, save_dictionary, play_video\n",
    "from pathlib import Path\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the filenames\n",
    "all_filenames = []\n",
    "f = \"temporary_result_storage_2\"\n",
    "folders = os.listdir(f)\n",
    "for inner in folders:\n",
    "    path = Path(f) / inner\n",
    "    if path.is_dir():\n",
    "        filenames = os.listdir(path)\n",
    "        for filename in filenames:\n",
    "            if filename[-4:] == \"json\":\n",
    "                all_filenames.append(str(path / filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = BulkAnalyzer(all_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|â–ˆ         | 36/322 [00:08<01:04,  4.45it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m reload \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reload:\n\u001b[1;32m----> 3\u001b[0m     \u001b[43manalyzer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     save_dictionary(analyzer\u001b[38;5;241m.\u001b[39mall_results, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemp_loaded_results_2.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\clare\\OneDrive - Duke University\\Projects\\Re-imaging\\git\\HighSpeedFLFM\\hsflfm\\analysis\\bulk_analyzer.py:165\u001b[0m, in \u001b[0;36mBulkAnalyzer.load_results\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dim \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m    164\u001b[0m     peak_indices[:, dim] \u001b[38;5;241m=\u001b[39m result_manager\u001b[38;5;241m.\u001b[39mpeak_indices(dim\u001b[38;5;241m=\u001b[39mdim)\n\u001b[1;32m--> 165\u001b[0m     peak_displacements[:, dim] \u001b[38;5;241m=\u001b[39m \u001b[43mresult_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpeak_displacements\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    166\u001b[0m     peak_norm_displacements[:, dim] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    167\u001b[0m         result_manager\u001b[38;5;241m.\u001b[39mpeak_norm_displacements(dim\u001b[38;5;241m=\u001b[39mdim)\n\u001b[0;32m    168\u001b[0m     )\n\u001b[0;32m    170\u001b[0m start_locations_ant_mm \u001b[38;5;241m=\u001b[39m result_manager\u001b[38;5;241m.\u001b[39mpoint_start_locs_ant_mm\n",
      "File \u001b[1;32mc:\\Users\\clare\\OneDrive - Duke University\\Projects\\Re-imaging\\git\\HighSpeedFLFM\\hsflfm\\analysis\\bulk_analyzer.py:48\u001b[0m, in \u001b[0;36mResultManager.peak_displacements\u001b[1;34m(self, dim)\u001b[0m\n\u001b[0;32m     46\u001b[0m displacements \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrel_displacements[:, :, dim]\n\u001b[0;32m     47\u001b[0m p \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(displacements\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m---> 48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m displacements[p, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpeak_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m]\n",
      "File \u001b[1;32mc:\\Users\\clare\\OneDrive - Duke University\\Projects\\Re-imaging\\git\\HighSpeedFLFM\\hsflfm\\analysis\\bulk_analyzer.py:43\u001b[0m, in \u001b[0;36mResultManager.peak_indices\u001b[1;34m(self, dim)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpeak_indices\u001b[39m(\u001b[38;5;28mself\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m     42\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrel_displacements\n\u001b[1;32m---> 43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_peak_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mint)\n",
      "File \u001b[1;32mc:\\Users\\clare\\OneDrive - Duke University\\Projects\\Re-imaging\\git\\HighSpeedFLFM\\hsflfm\\analysis\\basic_functions.py:24\u001b[0m, in \u001b[0;36mget_peak_indices\u001b[1;34m(displacements)\u001b[0m\n\u001b[0;32m     22\u001b[0m line_d \u001b[38;5;241m=\u001b[39m derivs[i, strike_center:]\n\u001b[0;32m     23\u001b[0m stop_point \u001b[38;5;241m=\u001b[39m displacements\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mline_d\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39msign(value) \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39msign(line_d[\u001b[38;5;241m0\u001b[39m]):\n\u001b[0;32m     26\u001b[0m         stop_point \u001b[38;5;241m=\u001b[39m index \u001b[38;5;241m+\u001b[39m strike_center\n",
      "File \u001b[1;32mc:\\Users\\clare\\anaconda3\\envs\\hsflfm\\Lib\\site-packages\\torch\\_tensor.py:1119\u001b[0m, in \u001b[0;36mTensor.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_tracing_state():\n\u001b[0;32m   1111\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1112\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIterating over a tensor might cause the trace to be incorrect. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1113\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing a tensor of different shape won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt change the number of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1117\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m   1118\u001b[0m     )\n\u001b[1;32m-> 1119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munbind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "reload = True\n",
    "if reload:\n",
    "    analyzer.load_results()\n",
    "    save_dictionary(analyzer.all_results, \"temp_loaded_results_2.json\")\n",
    "else:\n",
    "\n",
    "    analyzer.all_results = load_dictionary(\"temp_loaded_results_2.json\")\n",
    "\n",
    "    for key, value in analyzer.all_results.items():\n",
    "\n",
    "        if key == \"specimen_number\":\n",
    "\n",
    "            analyzer.all_results[key] = np.asarray(value)\n",
    "\n",
    "            continue\n",
    "\n",
    "        analyzer.all_results[key] = torch.asarray(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at all results on the mesh\n",
    "\n",
    "p = analyzer.all_results[\"mesh_points\"]\n",
    "\n",
    "# jitter the points\n",
    "\n",
    "jitter = 100\n",
    "\n",
    "rand = (torch.rand(p.shape) - 0.5) * jitter\n",
    "\n",
    "p = p + rand\n",
    "\n",
    "v = analyzer.all_results[\"normalized_displacement\"][:, 2]\n",
    "\n",
    "v = convert_to_percentile(v)\n",
    "\n",
    "ResultPlotter.plot_mesh_with_points(\n",
    "    points=p,\n",
    "    opacity=0.1,\n",
    "    point_values=v,\n",
    "    points_on_surface=False,\n",
    "    marker_dict={\"size\": 2, \"colorscale\": \"Turbo\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram flow differences\n",
    "key = \"average_flow_error\"\n",
    "all_flow = torch.mean(torch.abs(analyzer.all_results[key]), axis=1)\n",
    "all_flow, _ = torch.sort(all_flow)\n",
    "\n",
    "# cut-off at some percentile\n",
    "cutoff = 0.995\n",
    "cutoff_index = int(len(all_flow) * cutoff)\n",
    "\n",
    "bins = plt.hist(all_flow[:cutoff_index], bins=50, alpha=0.5, label=\"all 3 cameras\")\n",
    "width = bins[1][1] - bins[1][0]\n",
    "\n",
    "# add in the top two\n",
    "flow = analyzer.get_top_values(key)\n",
    "flow, _ = torch.sort(torch.mean(torch.abs(flow), axis=1))\n",
    "flow = flow[:cutoff_index]\n",
    "bins = np.arange(min(flow), max(flow) + width, width)\n",
    "_ = plt.hist(flow, bins=bins, alpha=0.5, label=\"top 2 cameras\")\n",
    "\n",
    "plt.legend()\n",
    "plt.ylabel(\"Point Count\")\n",
    "plt.xlabel(\"Flow Error (pixels)\")\n",
    "plt.title(\"Flow error in region around strike\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram flow differences\n",
    "key = \"average_flow_sq\"\n",
    "all_flow = torch.mean(torch.abs(analyzer.all_results[key]), axis=1)\n",
    "all_flow, _ = torch.sort(all_flow)\n",
    "\n",
    "# cut-off at some percentile\n",
    "cutoff = 0.95\n",
    "cutoff_index = int(len(all_flow) * cutoff)\n",
    "\n",
    "bins = plt.hist(all_flow[:cutoff_index], bins=50, alpha=0.5, label=\"all 3 cameras\")\n",
    "width = bins[1][1] - bins[1][0]\n",
    "\n",
    "# # add in the top two\n",
    "flow = analyzer.get_top_values(key)\n",
    "flow, _ = torch.sort(torch.mean(torch.abs(flow), axis=1))\n",
    "# cut-off at some percentile\n",
    "cutoff = 0.99\n",
    "cutoff_index = int(len(all_flow) * cutoff)\n",
    "flow = flow[:cutoff_index]\n",
    "bins = np.arange(min(flow), max(flow) + width, width)\n",
    "_ = plt.hist(flow, bins=bins, alpha=0.5, label=\"top 2 cameras\")\n",
    "\n",
    "plt.legend()\n",
    "plt.ylabel(\"Point Count\")\n",
    "plt.xlabel(\"Flow Error (pixels)\")\n",
    "plt.title(\"Flow error in region around strike\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram flow differences\n",
    "key = \"average_huber_loss\"\n",
    "all_flow = torch.mean(torch.abs(analyzer.all_results[key]), axis=1)\n",
    "all_flow, _ = torch.sort(all_flow)\n",
    "\n",
    "# cut-off at some percentile\n",
    "cutoff = 0.995\n",
    "cutoff_index = int(len(all_flow) * cutoff)\n",
    "\n",
    "bins = plt.hist(all_flow[:cutoff_index], bins=50, alpha=0.5, label=\"all 3 cameras\")\n",
    "width = bins[1][1] - bins[1][0]\n",
    "\n",
    "# add in the top two\n",
    "flow = analyzer.get_top_values(key)\n",
    "flow, _ = torch.sort(torch.mean(torch.abs(flow), axis=1))\n",
    "flow = flow[:cutoff_index]\n",
    "bins = np.arange(min(flow), max(flow) + width, width)\n",
    "_ = plt.hist(flow, bins=bins, alpha=0.5, label=\"top 2 cameras\")\n",
    "\n",
    "plt.legend()\n",
    "plt.ylabel(\"Point Count\")\n",
    "plt.xlabel(\"Huber Loss\")\n",
    "plt.title(\"Huber Loss in region around strike\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentile differences between huber and flow in top 2 cameras\n",
    "p0 = analyzer.get_percentile(\"average_flow_error\", num_cams=2)\n",
    "p1 = analyzer.get_percentile(\"average_flow_sq\", num_cams=2)\n",
    "\n",
    "diffs = torch.abs(p1 - p0)\n",
    "\n",
    "_ = plt.hist(diffs, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentile differences between huber and flow in top 2 cameras\n",
    "huber_percentiles = analyzer.get_percentile(\"average_huber_loss\", num_cams=2)\n",
    "flow_percentiles = analyzer.get_percentile(\"average_flow_error\", num_cams=2)\n",
    "\n",
    "diffs = torch.abs(huber_percentiles - flow_percentiles)\n",
    "\n",
    "_ = plt.hist(diffs, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at some points with bad flow\n",
    "array = torch.mean(analyzer.get_top_values(\"average_flow_sq\", num_cams=2), axis=1)\n",
    "index = get_random_percentile_index(array.numpy(), 90, 100)\n",
    "\n",
    "specimen_number = analyzer.all_results[\"specimen_number\"][index]\n",
    "point_number = int(analyzer.all_results[\"point_number\"][index])\n",
    "strike_number = int(analyzer.all_results[\"strike_number\"][index])\n",
    "\n",
    "print(specimen_number, \"point\", point_number, \"strike\", strike_number)\n",
    "print(\"flow error: {:.5f} pixels\".format(array[index]))\n",
    "print(\"percentile: {:.0f}%\".format(100 * get_percentiles(array.numpy(), float(array[index]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.025**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specimen_number = \"20240507_OB_2\"\n",
    "# point_number = 31\n",
    "# strike_number = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = (\n",
    "    f\"temporary_result_storage_2/{specimen_number}/strike_{strike_number}_results.json\"\n",
    ")\n",
    "assert os.path.exists(filename)\n",
    "\n",
    "result_info = load_dictionary(filename)\n",
    "plotter = ResultPlotter(result_info)\n",
    "\n",
    "plotter.result_info[\"removed_points\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_diffs = plotter.result_manager.flow_diff_around_strike()\n",
    "_, sorted = sort_by_camera(flow_diffs[:, :, None], treat_individually=False)\n",
    "values = sorted.squeeze()[:, :2]\n",
    "values = torch.mean(values, axis=1)\n",
    "_ = plotter.scatter_values(values, highlight_point=point_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter.scatter_peak_disp(highlight_point=point_number, cmap=\"turbo\")\n",
    "plotter.scatter_peak_disp(highlight_point=point_number, cmap=\"turbo\", with_image=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plotter.plot_camera_weight(point_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plotter.plot_displacement(point_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter.show_flow_differences(point_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plotter.plot_all_displacement(highlight_point=point_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid = plotter.make_point_track_video(highlight_point=point_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_video(vid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid = plotter.get_arrow_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_video(vid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = plotter.result_manager.point_mesh_locations\n",
    "ResultPlotter.plot_mesh_with_points(points=m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look into more:\n",
    "# 20240507_OB_2 point 31 strike 10\n",
    "# 20240502_OB_6 29\n",
    "# 20240502_OB_2 alignment is super off \n",
    "\n",
    "# good examples\n",
    "# 20240418_OB_1 barely any movement but clear pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suggested threshold for being used in global movement calculation:\n",
    "# 0.025 average error with top two cameras\n",
    "# in region surrounding peak \n",
    "# or... maybe squared error of 0.0015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = 0 \n",
    "pn = np.asarray(result_info[\"point_numbers\"], dtype=int)\n",
    "p1 = np.asarray(result_info[\"strike1_match_points\"][cam])[pn, :2]\n",
    "p2 = np.asarray(result_info[\"match_points\"][cam])\n",
    "diff = np.linalg.norm(p1 - p2, axis=1)\n",
    "\n",
    "_ = plotter.scatter_values(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hsflfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
