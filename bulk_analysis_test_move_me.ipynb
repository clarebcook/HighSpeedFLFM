{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hsflfm.analysis import (\n",
    "    ResultManager,\n",
    "    ResultPlotter,\n",
    "    BulkAnalyzer,\n",
    "    convert_to_percentile,\n",
    "    get_random_percentile_index,\n",
    "    sort_by_camera,\n",
    "    get_percentiles,\n",
    ")\n",
    "from hsflfm.util import MetadataManager\n",
    "from scipy.spatial import cKDTree\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from hsflfm.util import load_dictionary, save_dictionary, play_video\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the filenames\n",
    "all_filenames = []\n",
    "f = \"temporary_result_storage_4\"\n",
    "folders = os.listdir(f)\n",
    "for inner in folders:\n",
    "    path = Path(f) / inner\n",
    "    if path.is_dir():\n",
    "        filenames = os.listdir(path)\n",
    "        for filename in filenames:\n",
    "            if filename[-4:] == \"json\":\n",
    "                all_filenames.append(str(path / filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = BulkAnalyzer(all_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points = []\n",
    "is_new_strike = np.zeros(len(all_filenames), dtype=int) \n",
    "prev_name = None \n",
    "names = []\n",
    "for i, f in enumerate(tqdm(all_filenames)):\n",
    "    info = load_dictionary(f)\n",
    "    num_points.append(len(info[\"point_numbers\"]))\n",
    "    name = info[\"specimen_number\"]\n",
    "    is_new_strike[i] =(name != prev_name)\n",
    "\n",
    "    prev_name = name \n",
    "    names.append(name)\n",
    "\n",
    "    #if i > 30:\n",
    "    #    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload = True\n",
    "res_filename = \"temp_loaded_results_4.json\"\n",
    "\n",
    "\n",
    "if reload:\n",
    "\n",
    "    analyzer.load_results()\n",
    "    save_dictionary(analyzer.all_results, res_filename)\n",
    "else:\n",
    "    analyzer.all_results = load_dictionary(res_filename)\n",
    "\n",
    "    for key, value in analyzer.all_results.items():\n",
    "\n",
    "        if key == \"specimen_number\":\n",
    "\n",
    "            analyzer.all_results[key] = np.asarray(value)\n",
    "\n",
    "            continue\n",
    "\n",
    "        analyzer.all_results[key] = torch.asarray(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at all results on the mesh\n",
    "\n",
    "p = analyzer.all_results[\"mesh_points\"]\n",
    "\n",
    "# jitter the points\n",
    "\n",
    "jitter = 10000\n",
    "\n",
    "rand = (torch.rand(p.shape) - 0.5) * jitter\n",
    "\n",
    "p = p + rand\n",
    "\n",
    "v = analyzer.all_results[\"normalized_displacement\"][:, 2]\n",
    "good_point_indices = torch.where(analyzer.error_scores < 0.0015)[0]\n",
    "\n",
    "v = v[good_point_indices]\n",
    "\n",
    "v = convert_to_percentile(v)\n",
    "\n",
    "\n",
    "\n",
    "ResultPlotter.plot_mesh_with_points(\n",
    "    points=p[good_point_indices],\n",
    "    opacity=0.0,\n",
    "    point_values=v,\n",
    "    points_on_surface=False,\n",
    "    marker_dict={\"size\": 1, \"colorscale\": \"Turbo\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try out getting a strength score for each video\n",
    "specimen_names = MetadataManager.all_specimen_numbers()\n",
    "scores = []\n",
    "names = []\n",
    "strike_nums = []\n",
    "\n",
    "all_error_scores = analyzer.error_scores\n",
    "for name in tqdm(specimen_names):\n",
    "    strike_numbers = MetadataManager(name).strike_numbers\n",
    "    for strike_number in strike_numbers:\n",
    "        idx = analyzer.get_specimen_indices(name, strike_number=strike_number)\n",
    "\n",
    "        if len(idx) < 15:\n",
    "            print(f\"skipping {name} strike {strike_number}, {len(idx)} points\")\n",
    "            continue\n",
    "\n",
    "        k = 15\n",
    "        _, neighbor_indices = analyzer.get_closest_point_indices(\n",
    "            k=k, indices=good_point_indices\n",
    "        )\n",
    "        neighbor_indices = neighbor_indices[idx]\n",
    "\n",
    "        ratios = np.zeros(neighbor_indices.shape[0])\n",
    "        for pi, neighbor_index in enumerate(neighbor_indices):\n",
    "            displacements = analyzer.all_results[\"displacement\"][neighbor_index]\n",
    "            disp_norm = displacements[:, :2]\n",
    "\n",
    "            point_disp = analyzer.all_results[\"displacement\"][idx[pi]][2]\n",
    "            ratios[pi] = torch.abs(point_disp) / torch.mean(torch.abs(disp_norm))\n",
    "\n",
    "        # only use points below the threshold\n",
    "        error_values = all_error_scores[idx]\n",
    "        strike_good_point_indices = error_values < 0.0015\n",
    "\n",
    "        if torch.count_nonzero(strike_good_point_indices) < 15:\n",
    "            print(\n",
    "                f\"skipping {name} strike {strike_number}, {torch.count_nonzero(strike_good_point_indices)} good points\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        #if \"20240503_OB_3\" in name:\n",
    "        #    # will address this later, I just know this video is bad\n",
    "        #    continue\n",
    "\n",
    "        score = np.mean(ratios[strike_good_point_indices])\n",
    "        scores.append(score)\n",
    "        strike_nums.append(strike_number)\n",
    "        names.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(scores, bins=50)\n",
    "# plt.xlim(0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.nanargmax(scores)\n",
    "name = names[n]\n",
    "num = strike_nums[n]\n",
    "\n",
    "strike_indices = analyzer.get_specimen_indices(name, num)\n",
    "print(name, \"strike\", num, \"score:\", scores[n])\n",
    "\n",
    "filename = f\"temporary_result_storage_4/{name}/strike_{num}_results.json\"\n",
    "assert os.path.exists(filename)\n",
    "\n",
    "result_info = load_dictionary(filename)\n",
    "plotter = ResultPlotter(result_info)\n",
    "strike_good_point_indices = all_error_scores[strike_indices] < 0.0015\n",
    "\n",
    "plotter.scatter_peak_disp(highlight_point=10)\n",
    "\n",
    "fig = plotter.scatter_values(analyzer.error_scores[strike_indices])\n",
    "fig.suptitle(\"error scores\")\n",
    "ant_start_locs = plotter.result_manager.point_start_locs_ant_mm\n",
    "ant_start_locs = ant_start_locs[np.where(~strike_good_point_indices)]\n",
    "ax = fig.axes[0]\n",
    "ax.scatter(ant_start_locs[:, 1], ant_start_locs[:, 0], marker=\"x\", color=\"red\", s=7)\n",
    "\n",
    "\n",
    "fig = plotter.plot_all_displacement(highlight_point=10)\n",
    "# highlight the points below the error threshold in blue\n",
    "ax = fig.axes[0]\n",
    "bad_disp = plotter.result_manager.rel_displacements[~strike_good_point_indices]\n",
    "for p in bad_disp:\n",
    "    plt.plot(p[:, 2] * 1e3, \"--\", color=(0.5, 0.5, 1))\n",
    "# plotter.plot_displacement(10)\n",
    "\n",
    "vid = plotter.get_arrow_video(cam_num=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_video(vid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(vid[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram flow differences\n",
    "key = \"average_flow_error\"\n",
    "all_flow = torch.mean(torch.abs(analyzer.all_results[key]), axis=1)\n",
    "all_flow, _ = torch.sort(all_flow)\n",
    "\n",
    "# cut-off at some percentile\n",
    "cutoff = 0.995\n",
    "cutoff_index = int(len(all_flow) * cutoff)\n",
    "\n",
    "bins = plt.hist(all_flow[:cutoff_index], bins=50, alpha=0.5, label=\"all 3 cameras\")\n",
    "width = bins[1][1] - bins[1][0]\n",
    "\n",
    "# add in the top two\n",
    "flow = analyzer.get_top_values(key)\n",
    "flow, _ = torch.sort(torch.mean(torch.abs(flow), axis=1))\n",
    "flow = flow[:cutoff_index]\n",
    "bins = np.arange(min(flow), max(flow) + width, width)\n",
    "_ = plt.hist(flow, bins=bins, alpha=0.5, label=\"top 2 cameras\")\n",
    "\n",
    "plt.legend()\n",
    "plt.ylabel(\"Point Count\")\n",
    "plt.xlabel(\"Flow Error (pixels)\")\n",
    "plt.title(\"Flow error in region around strike\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram flow differences\n",
    "key = \"average_flow_sq\"\n",
    "all_flow = torch.mean(torch.abs(analyzer.all_results[key]), axis=1)\n",
    "all_flow, _ = torch.sort(all_flow)\n",
    "\n",
    "# cut-off at some percentile\n",
    "cutoff = 0.95\n",
    "cutoff_index = int(len(all_flow) * cutoff)\n",
    "\n",
    "bins = plt.hist(all_flow[:cutoff_index], bins=50, alpha=0.5, label=\"all 3 cameras\")\n",
    "width = bins[1][1] - bins[1][0]\n",
    "\n",
    "# # add in the top two\n",
    "flow = analyzer.get_top_values(key)\n",
    "flow, _ = torch.sort(torch.mean(torch.abs(flow), axis=1))\n",
    "# cut-off at some percentile\n",
    "cutoff = 0.99\n",
    "cutoff_index = int(len(all_flow) * cutoff)\n",
    "flow = flow[:cutoff_index]\n",
    "bins = np.arange(min(flow), max(flow) + width, width)\n",
    "_ = plt.hist(flow, bins=bins, alpha=0.5, label=\"top 2 cameras\")\n",
    "\n",
    "plt.legend()\n",
    "plt.ylabel(\"Point Count\")\n",
    "plt.xlabel(\"Flow Error (pixels)\")\n",
    "plt.title(\"Flow error in region around strike\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram flow differences\n",
    "key = \"average_huber_loss\"\n",
    "all_flow = torch.mean(torch.abs(analyzer.all_results[key]), axis=1)\n",
    "all_flow, _ = torch.sort(all_flow)\n",
    "\n",
    "# cut-off at some percentile\n",
    "cutoff = 0.995\n",
    "cutoff_index = int(len(all_flow) * cutoff)\n",
    "\n",
    "bins = plt.hist(all_flow[:cutoff_index], bins=50, alpha=0.5, label=\"all 3 cameras\")\n",
    "width = bins[1][1] - bins[1][0]\n",
    "\n",
    "# add in the top two\n",
    "flow = analyzer.get_top_values(key)\n",
    "flow, _ = torch.sort(torch.mean(torch.abs(flow), axis=1))\n",
    "flow = flow[:cutoff_index]\n",
    "bins = np.arange(min(flow), max(flow) + width, width)\n",
    "_ = plt.hist(flow, bins=bins, alpha=0.5, label=\"top 2 cameras\")\n",
    "\n",
    "plt.legend()\n",
    "plt.ylabel(\"Point Count\")\n",
    "plt.xlabel(\"Huber Loss\")\n",
    "plt.title(\"Huber Loss in region around strike\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentile differences between huber and flow in top 2 cameras\n",
    "p0 = analyzer.get_percentile(\"average_flow_error\", num_cams=2)\n",
    "p1 = analyzer.get_percentile(\"average_flow_sq\", num_cams=2)\n",
    "\n",
    "diffs = torch.abs(p1 - p0)\n",
    "\n",
    "_ = plt.hist(diffs, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentile differences between huber and flow in top 2 cameras\n",
    "huber_percentiles = analyzer.get_percentile(\"average_huber_loss\", num_cams=2)\n",
    "flow_percentiles = analyzer.get_percentile(\"average_flow_error\", num_cams=2)\n",
    "\n",
    "diffs = torch.abs(huber_percentiles - flow_percentiles)\n",
    "\n",
    "_ = plt.hist(diffs, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at which points vary the most from their neighbors\n",
    "points = analyzer.all_results[\"start_locations_std\"]\n",
    "values = analyzer.all_results[\"normalized_displacement\"]\n",
    "tree = cKDTree(points)\n",
    "k = 25\n",
    "distances, indices = tree.query(points, k=k + 1)\n",
    "# exclude self\n",
    "neighbor_indices = indices[:, 1:]\n",
    "\n",
    "neighbor_avgs = values[neighbor_indices].mean(axis=1)\n",
    "difference = values - neighbor_avgs\n",
    "neighbor_diff_z = difference[:, 2]\n",
    "\n",
    "_ = plt.hist(torch.abs(neighbor_diff_z), bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at which points vary the most from their neighbors, not normalized\n",
    "points = analyzer.all_results[\"start_locations_std\"]\n",
    "values = analyzer.all_results[\"displacement\"]\n",
    "tree = cKDTree(points)\n",
    "k = 25\n",
    "distances, indices = tree.query(points, k=k + 1)\n",
    "# exclude self\n",
    "neighbor_indices = indices[:, 1:]\n",
    "\n",
    "neighbor_avgs = values[neighbor_indices].mean(axis=1)\n",
    "difference = values - neighbor_avgs\n",
    "neighbor_diff_z = difference[:, 2]\n",
    "\n",
    "_ = plt.hist(torch.abs(neighbor_diff_z), bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at some points with bad flow\n",
    "array = torch.mean(analyzer.get_top_values(\"average_flow_sq\", num_cams=2), axis=1)\n",
    "array = neighbor_diff_z\n",
    "\n",
    "\n",
    "index = get_random_percentile_index(array.numpy(), 95, 100)\n",
    "\n",
    "specimen_number = analyzer.all_results[\"specimen_number\"][index]\n",
    "point_number = int(analyzer.all_results[\"point_number\"][index])\n",
    "strike_number = int(analyzer.all_results[\"strike_number\"][index])\n",
    "\n",
    "print(specimen_number, \"point\", point_number, \"strike\", strike_number)\n",
    "print(\"flow error: {:.5f} pixels\".format(array[index]))\n",
    "print(\n",
    "    \"percentile: {:.0f}%\".format(\n",
    "        100 * get_percentiles(array.numpy(), float(array[index]))\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specimen_number = \"20240503_OB_3\"\n",
    "# point_number = 15\n",
    "# strike_number = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specimen_number = str(analyzer.all_results[\"specimen_number\"][0])\n",
    "# point_number = 30\n",
    "# strike_number = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the indices related to this strike\n",
    "indices1 = np.where(analyzer.all_results[\"specimen_number\"] == specimen_number)[0]\n",
    "indices2 = np.where(analyzer.all_results[\"strike_number\"] == strike_number)[0]\n",
    "\n",
    "indices = np.intersect1d(indices1, indices2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = (\n",
    "    f\"temporary_result_storage_3/{specimen_number}/strike_{strike_number}_results.json\"\n",
    ")\n",
    "assert os.path.exists(filename)\n",
    "\n",
    "result_info = load_dictionary(filename)\n",
    "\n",
    "\n",
    "\n",
    "plotter = ResultPlotter(result_info)\n",
    "\n",
    "\n",
    "\n",
    "# result_info[\"point_numbers\"]\n",
    "\n",
    "\n",
    "\n",
    "# plotter.result_info[\"removed_points\"]\n",
    "\n",
    "\n",
    "\n",
    "# plotter.result_info[\"points_used_in_gm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = torch.mean(analyzer.get_top_values(\"average_flow_sq\", num_cams=2), axis=1)\n",
    "error_values = array[indices]\n",
    "good_point_indices = error_values < 0.0015\n",
    "fig = plotter.scatter_values(error_values, highlight_point=point_number)\n",
    "\n",
    "# mark the points above the error threshold with a red x\n",
    "ant_start_locs = plotter.result_manager.point_start_locs_ant_mm\n",
    "ant_start_locs = ant_start_locs[np.where(~good_point_indices)]\n",
    "ax = fig.axes[0]\n",
    "ax.scatter(ant_start_locs[:, 1], ant_start_locs[:, 0], marker=\"x\", color=\"red\", s=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_diffs = plotter.result_manager.flow_diff_around_strike()\n",
    "_, sorted = sort_by_camera(flow_diffs[:, :, None], treat_individually=False)\n",
    "values = sorted.squeeze()[:, :2]\n",
    "values = torch.mean(values, axis=1)\n",
    "_ = plotter.scatter_values(values, highlight_point=point_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter.scatter_peak_disp(highlight_point=point_number, cmap=\"turbo\")\n",
    "\n",
    "# mark the points above the error threshold with a black x\n",
    "ant_start_locs = plotter.result_manager.point_start_locs_ant_mm\n",
    "ant_start_locs = ant_start_locs[np.where(~good_point_indices)]\n",
    "ax = plt.gca()\n",
    "ax.scatter(ant_start_locs[:, 1], ant_start_locs[:, 0], marker=\"x\", color=\"black\", s=15)\n",
    "\n",
    "plotter.scatter_peak_disp(highlight_point=point_number, cmap=\"turbo\", with_image=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plotter.plot_camera_weight(point_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plotter.plot_displacement(point_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter.show_flow_differences(point_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plotter.plot_all_displacement(highlight_point=point_number)\n",
    "\n",
    "# highlight the points below the error threshold in blue\n",
    "ax = fig.axes[0]\n",
    "bad_disp = plotter.result_manager.rel_displacements[~good_point_indices]\n",
    "for p in bad_disp:\n",
    "    plt.plot(p[:, 2] * 1e3, \"--\", color=(0.5, 0.5, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid = plotter.make_point_track_video(highlight_point=point_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_video(vid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid = plotter.get_arrow_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_video(vid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = plotter.result_manager.point_mesh_locations\n",
    "ResultPlotter.plot_mesh_with_points(points=m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look into more:\n",
    "# 20240507_OB_2 point 31 strike 10\n",
    "# 20240502_OB_6 29\n",
    "# 20240502_OB_2 alignment is super off\n",
    "# \"20220422_OB_1\" something is wrong with strikes 6 and 7\n",
    "\n",
    "# good examples\n",
    "# 20240418_OB_1 barely any movement but clear pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suggested threshold for being used in global movement calculation:\n",
    "# 0.025 average error with top two cameras\n",
    "# in region surrounding peak\n",
    "# or... maybe squared error of 0.0015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at results on the mesh\n",
    "# with ONLY points below the error threshold\n",
    "array = torch.mean(analyzer.get_top_values(\"average_flow_sq\", num_cams=2), axis=1)\n",
    "good_indices = array < 0.0015\n",
    "# good_indices = ~good_indices\n",
    "p = analyzer.all_results[\"mesh_points\"][good_indices]\n",
    "\n",
    "# jitter the points\n",
    "\n",
    "jitter = 1000\n",
    "rand = (torch.rand(p.shape) - 0.5) * jitter\n",
    "p = p + rand\n",
    "v = analyzer.all_results[\"normalized_displacement\"][good_indices, 2]\n",
    "v = convert_to_percentile(v)\n",
    "\n",
    "ResultPlotter.plot_mesh_with_points(\n",
    "    points=p,\n",
    "    opacity=0.1,\n",
    "    point_values=v,\n",
    "    points_on_surface=False,\n",
    "    marker_dict={\"size\": 2, \"colorscale\": \"bluered\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scattering the same stuff\n",
    "array = torch.mean(analyzer.get_top_values(\"average_flow_sq\", num_cams=2), axis=1)\n",
    "good_indices = array < 0.0015\n",
    "\n",
    "p = analyzer.all_results[\"start_locations_std\"]\n",
    "jitter = 30\n",
    "rand = (torch.rand(p.shape) - 0.5) * jitter\n",
    "p = p + rand\n",
    "\n",
    "v = analyzer.all_results[\"normalized_displacement\"][:, 2]\n",
    "v = convert_to_percentile(v)\n",
    "plt.scatter(\n",
    "    p[good_indices, 1],\n",
    "    p[good_indices, 0],\n",
    "    s=1.5,\n",
    "    c=v[good_indices],\n",
    "    cmap=\"coolwarm\",\n",
    "    clim=(0, 100),\n",
    ")\n",
    "ax = plt.gca()\n",
    "ax.set_aspect(\"equal\")\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(\n",
    "    p[~good_indices, 1],\n",
    "    p[~good_indices, 0],\n",
    "    s=1.5,\n",
    "    c=v[~good_indices],\n",
    "    cmap=\"coolwarm\",\n",
    "    clim=(0, 100),\n",
    ")\n",
    "ax = plt.gca()\n",
    "ax.set_aspect(\"equal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some strikes that don't look good\n",
    "# based on differences from nearby points\n",
    "# 20220427_OB_3 strike 2 (this whole ant might be weird, look closer)\n",
    "# we know 20240503_OB_3 was having issues\n",
    "# check on what other samples had to change the error threshold\n",
    "# for global movement calculation\n",
    "\n",
    "# 20220427_OB_4 is interesting because most of the seleted points\n",
    "# are in the saddle - probably makes computing normalized movement weird\n",
    "# might want to think about other ways to get like a normalized score...\n",
    "# like thinking about how much on average the points in that strike deviate\n",
    "# from the expected strike strength in that region\n",
    "\n",
    "# 20240507_OB_3 strike 9, example of a super weak strike that doesn't follow\n",
    "# expected patterns. a lot of strikes from this ant look weak... investigate more\n",
    "\n",
    "# you need to check on how many points made it over in later strikes\n",
    "# for instance 20240502_OB_3 strike 9 has very few points\n",
    "# it could be worth not actually dropping points that aren't CRAZY off\n",
    "# but just saving information about the liklihood that the track was good\n",
    "# and maybe just acknowledging in that way that it COULD be a different point\n",
    "\n",
    "# 20240506_OB_7 strike 2, take a look at alignment here. Points are all really low\n",
    "\n",
    "# 20240503_OB_3 strike 1 in particular, something is really weird here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2024/11/13\n",
    "# goal for today:\n",
    "# define some initial metric for a strength score\n",
    "# then you can use that to do normalized strength measurements\n",
    "# and also threshold strikes based on the strength score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hsflfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
